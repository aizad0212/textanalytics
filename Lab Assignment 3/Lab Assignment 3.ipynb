{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7451eec2-445e-4596-8248-7c15bc8921b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CISB5123 - TEXT ANALYTICS (SECTION 03)\n",
    "# AHMAD AMIRUL AIZAD BIN ROSMADI [IS01082507]\n",
    "# MUHAMMMAD NABIL BIN MUHAMMAD [IS01082117]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7ae8742-09e7-43e6-87c3-44f6ddfb91ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e27237f-c93e-4b51-a65f-108b9df0b6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"news_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8554f6e5-b8cc-4dcb-b0af-34ef92f370d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary NLTK datasets\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a10f2fa0-3dc8-4afc-8a4f-a1b38c887a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the lemmatizer and stemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbfa6405-c4b3-43ae-9bb0-b31e1cdbd187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows where the 'text' column has no value (empty strings or NaN)\n",
    "df_cleaned = df[df['text'].notna() & (df['text'] != '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef11361c-696b-4594-9159-5a9801b749c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for text pre-processing\n",
    "def preprocess_text_simple(text):\n",
    "    # Remove non-alphabetic characters and convert to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
    "    # Tokenize text by splitting into words\n",
    "    words = text.split()\n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "    # Return cleaned text\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca56d538-d587-457a-915e-87f413d0e1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_22288\\2671912900.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['cleaned_text'] = df_cleaned['text'].apply(preprocess_text_simple)\n"
     ]
    }
   ],
   "source": [
    "# Apply the pre-processing function to the filtered 'text' column\n",
    "df_cleaned['cleaned_text'] = df_cleaned['text'].apply(preprocess_text_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "370b61ca-3e45-4ddd-857e-0e0cb4d61762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the cleaned text into words\n",
    "tokenized_text = [text.split() for text in df_cleaned['cleaned_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52e89f31-8356-4d9f-a984-adbca7af7386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary representation of the documents\n",
    "dictionary = corpora.Dictionary(tokenized_text)\n",
    "\n",
    "# Filter out words that occur in less than 5 documents or more than 50% of the documents\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "\n",
    "# Create a bag of words corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_text]\n",
    "\n",
    "# Build the LDA model\n",
    "lda_model = LdaModel(corpus, num_topics=4, id2word=dictionary, passes=5, iterations=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "531fea76-ae4c-4093-9ee9-21b946f59bd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Terms for Each Topic:\n",
      "Topic 0:\n",
      " - \"b\" (weight: 0.011)\n",
      " - \"db\" (weight: 0.011)\n",
      " - \"chip\" (weight: 0.009)\n",
      " - \"one\" (weight: 0.008)\n",
      " - \"would\" (weight: 0.008)\n",
      " - \"clipper\" (weight: 0.007)\n",
      " - \"use\" (weight: 0.006)\n",
      " - \"encryption\" (weight: 0.006)\n",
      " - \"get\" (weight: 0.006)\n",
      " - \"like\" (weight: 0.005)\n",
      "\n",
      "Topic 1:\n",
      " - \"people\" (weight: 0.011)\n",
      " - \"would\" (weight: 0.010)\n",
      " - \"one\" (weight: 0.009)\n",
      " - \"dont\" (weight: 0.007)\n",
      " - \"think\" (weight: 0.006)\n",
      " - \"know\" (weight: 0.006)\n",
      " - \"like\" (weight: 0.005)\n",
      " - \"us\" (weight: 0.005)\n",
      " - \"say\" (weight: 0.005)\n",
      " - \"even\" (weight: 0.004)\n",
      "\n",
      "Topic 2:\n",
      " - \"x\" (weight: 0.024)\n",
      " - \"key\" (weight: 0.009)\n",
      " - \"use\" (weight: 0.008)\n",
      " - \"file\" (weight: 0.007)\n",
      " - \"information\" (weight: 0.006)\n",
      " - \"available\" (weight: 0.005)\n",
      " - \"program\" (weight: 0.005)\n",
      " - \"anonymous\" (weight: 0.005)\n",
      " - \"system\" (weight: 0.004)\n",
      " - \"email\" (weight: 0.004)\n",
      "\n",
      "Topic 3:\n",
      " - \"q\" (weight: 0.007)\n",
      " - \"president\" (weight: 0.006)\n",
      " - \"new\" (weight: 0.005)\n",
      " - \"mr\" (weight: 0.005)\n",
      " - \"government\" (weight: 0.004)\n",
      " - \"stephanopoulos\" (weight: 0.004)\n",
      " - \"year\" (weight: 0.004)\n",
      " - \"first\" (weight: 0.004)\n",
      " - \"would\" (weight: 0.004)\n",
      " - \"last\" (weight: 0.003)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Top Terms for Each Topic:\")\n",
    "for idx, topic in lda_model.print_topics():\n",
    "    print(f\"Topic {idx}:\")\n",
    "    terms = [term.strip() for term in topic.split(\"+\")]\n",
    "    for term in terms:\n",
    "        weight, word = term.split(\"*\")\n",
    "        print(f\" - {word.strip()} (weight: {weight.strip()})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52b4062-049e-4789-b62b-db55f31277da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretion\n",
    "'''\n",
    "Topic 0 \n",
    "- Focus on Technology and Encryption.\n",
    "- This topic seems to be related to computing systems, encryption technologies, and data management. The terms suggest a focus on \n",
    "databases, encryption keys, and hardware/software components, with a possible focus on security and system protection.\n",
    "\n",
    "Topic 1\n",
    "- Focus on General opnions.\n",
    "- This topic appears to focus on opinions or discussions. The frequent use of the list word suggests personal opinions or statements in conversations. \n",
    "\n",
    "Topic 2\n",
    "- Focus on Information Systems security.\n",
    "- This topic seems to be focused on data security and information systems. The key terms suggest that the topic revolves around encryption, \n",
    "file security, programming, and anonymous systems.\n",
    "\n",
    "Topic 3\n",
    "- Focus on Political\n",
    "- This topic is most likely centered around politics or governmental matters, with terms like \"president\", \"government\".\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
